{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\GitHub\\\\NLP-Emotion-Classification-End-to-End-Project\\\\NLP-Emotion-Classification-End-to-End-Project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataCleaningConfig:\n",
    "    root_dir: Path\n",
    "    data_ingestion_dir: Path\n",
    "    cleaned_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.emotionClassification.constants import *\n",
    "from src.emotionClassification.utils.common import read_yaml_file, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Class to manage the configuration parameters and initialize configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path: Path = CONFIG_FILE_PATH,\n",
    "        params_file_path: Path = PARAMS_FILE_PATH,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with the provided file paths.\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = read_yaml_file(config_file_path)\n",
    "        self.params = read_yaml_file(params_file_path)\n",
    "\n",
    "        create_directories(filepath_list=[self.config.artifacts_root, self.config.data_cleaning.cleaned_dir,])\n",
    "\n",
    "    def get_data_cleaning_config_and_params(self) -> DataCleaningConfig:\n",
    "        \"\"\"\n",
    "        Return the DataCleaningConfig object initialized with the configuration parameters.\n",
    "        \"\"\"\n",
    "        config = self.config.data_cleaning\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return DataCleaningConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_ingestion_dir=config.data_ingestion_dir,\n",
    "            cleaned_dir=config.cleaned_dir,\n",
    "        ), self.params.data_cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from box import ConfigBox\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import contractions\n",
    "from autocorrect import Speller\n",
    "import pandas as pd\n",
    "import string\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from src.emotionClassification.logging import logger\n",
    "\n",
    "\n",
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    Represents a data cleaning process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataCleaningConfig, params: ConfigBox) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataCleaning class with the given configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "    def clean_data(self, batch: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Cleans the text data in the input batch.\n",
    "        Args:\n",
    "            batch: Input batch containing the text data.\n",
    "        Returns:\n",
    "            The input batch with preprocessed text.\n",
    "        \"\"\"\n",
    "        text = batch['Tweet']\n",
    "        series = pd.Series(text) # To Speed up operations on batch\n",
    "        \n",
    "        # Apply preprocessing steps using vectorized operations\n",
    "        series = series.apply(lambda x: contractions.fix(x)) # Expand contractions\n",
    "        series = series.str.lower() # Lowercase\n",
    "        series = series.str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)  # Remove URLs\n",
    "        series = series.str.replace(r'@\\w+', '', regex=True)  # Remove mentions\n",
    "        series = series.str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "        series = series.str.replace(r'(.)\\1+', r'\\1\\1', regex=True)  # Handle elongation\n",
    "        series = series.str.replace(f'[{string.punctuation}]', '', regex=True) # Remove punctuation\n",
    "\n",
    "    #     # Remove stopwords\n",
    "    #     def remove_stopwords(text):\n",
    "    #         words = text.split()\n",
    "    #         filtered_words = [word for word in words if word not in stop_words]\n",
    "    #         return ' '.join(filtered_words)\n",
    "    #     series = series.apply(remove_stopwords)\n",
    "        \n",
    "        series = series.apply(lambda x: emoji.demojize(x))  # Convert emojis to text\n",
    "        text = series.tolist()\n",
    "        batch['Tweet'] = text\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    def save_and_return_cleaned_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        Save the cleaned data to disk.\n",
    "        \"\"\"\n",
    "        loaded_data = load_from_disk(self.config.data_ingestion_dir)\n",
    "        cleaned_data = loaded_data.map(self.clean_data, batched=True, batch_size=self.params.batch_size)\n",
    "        cleaned_data.save_to_disk(self.config.cleaned_dir)\n",
    "\n",
    "        return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaningPipeline:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the pipeline\"\"\"\n",
    "        pass\n",
    "\n",
    "    def main(self) -> dict:\n",
    "        \"\"\"Execute the pipeline\"\"\"\n",
    "\n",
    "        config = ConfigurationManager()\n",
    "        data_cleaning_config, data_cleaning_params = config.get_data_cleaning_config_and_params()\n",
    "\n",
    "        data_cleaning = DataCleaning(config=data_cleaning_config, params=data_cleaning_params)\n",
    "        cleaned_data = data_cleaning.save_and_return_cleaned_data()\n",
    "\n",
    "        return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-29 18:51:11,031: INFO: 1559613091: >>>> Stage Data Cleaning/Preprocessing Started <<<<]\n",
      "[2024-08-29 18:51:11,049: INFO: common: YAML file config\\config.yaml loaded successfully!]\n",
      "[2024-08-29 18:51:11,059: INFO: common: YAML file params.yaml loaded successfully!]\n",
      "[2024-08-29 18:51:11,064: INFO: common: Directory artifacts already exists!]\n",
      "[2024-08-29 18:51:11,064: INFO: common: Directory artifacts/data_cleaning/sem_eval_2018_task_1 already exists!]\n",
      "[2024-08-29 18:51:11,069: INFO: common: Directory artifacts/data_cleaning already exists!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6838/6838 [00:04<00:00, 1628.44 examples/s]\n",
      "Map: 100%|██████████| 3259/3259 [00:01<00:00, 2406.60 examples/s]\n",
      "Map: 100%|██████████| 886/886 [00:00<00:00, 2167.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6838/6838 [00:00<00:00, 110204.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3259/3259 [00:00<00:00, 112284.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 886/886 [00:00<00:00, 61848.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-29 18:51:24,404: INFO: 1559613091: >>>> Stage Data Cleaning/Preprocessing Completed Successfully <<<<]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.emotionClassification.logging import logger\n",
    "\n",
    "STAGE_NAME = \"Data Cleaning/Preprocessing\"\n",
    "\n",
    "try:\n",
    "    logger.info(f\">>>> Stage {STAGE_NAME} Started <<<<\")\n",
    "    data_cleaning = DataCleaningPipeline()\n",
    "    cleaned_data = data_cleaning.main()\n",
    "    logger.info(f\">>>> Stage {STAGE_NAME} Completed Successfully <<<<\")\n",
    "except Exception as e:\n",
    "    logger.error(f\">>>> Stage {STAGE_NAME} Failed <<<<\")\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
